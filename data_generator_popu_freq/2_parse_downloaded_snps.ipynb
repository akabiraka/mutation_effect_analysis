{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "home_dir = \"../\"\n",
    "\n",
    "import pandas as pd\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filehandle(filepath, colnames_as_tab_sep_str):\n",
    "    if os.path.exists(filepath):\n",
    "        os.remove(filepath)\n",
    "    fhandle = open(filepath, mode=\"w\")\n",
    "    fhandle.write(f\"{colnames_as_tab_sep_str}\") # write column names\n",
    "    return fhandle\n",
    "\n",
    "    #     fhandle = open(filepath, mode=\"a\")\n",
    "    # else:\n",
    "    #     fhandle = open(filepath, mode=\"w\")\n",
    "    #     fhandle.write(f\"{colnames_as_tab_sep_str}\") # write column names\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps_filepath = home_dir+\"data/dbsnp/parsed/snps.tsv\"\n",
    "snps_cols = \"SNP_ID\\tSNP_CLASS\\tCLINICAL_SIGNIFICANCE\\tACC\\tCHR\\tCHRPOS\\tSPDI\\tFXN_CLASS\\tVALIDATED\\tTAX_ID\\tCREATEDATE\\tUPDATEDATE\\tDOCSUM\\tNP_VARIANTS\\tREF_ALLELE\\tALT_ALLELES\\n\"\n",
    "snps_fhandle = get_filehandle(snps_filepath, snps_cols)\n",
    "\n",
    "genes_filepath = home_dir+\"data/dbsnp/parsed/genes.tsv\"\n",
    "genes_cols = \"SNP_ID\\tGENE_NAME\\tGENE_ID\\n\"\n",
    "genes_fhandle = get_filehandle(genes_filepath, genes_cols)\n",
    "\n",
    "global_mafs_filepath = home_dir+\"data/dbsnp/parsed/global_mafs.tsv\"\n",
    "global_mafs_cols = \"SNP_ID\\tSTUDY\\tFREQ\\tALT_ALLELE\\tALT_FREQ\\tALT_POPU\\n\"\n",
    "global_mafs_fhandle = get_filehandle(global_mafs_filepath, global_mafs_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_document_summary(doc_sum):\n",
    "    snp_dict = xmltodict.parse(doc_sum)[\"DocumentSummary\"]\n",
    "    # print(snp_dict)\n",
    "\n",
    "    snp_id = \"rs\"+snp_dict[\"SNP_ID\"] # primary and foreign key of the tables\n",
    "\n",
    "    snps_fhandle.write(f'{snp_id}\\t') # col: SNP_ID\n",
    "    snps_fhandle.write(f'{snp_dict[\"SNP_CLASS\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"CLINICAL_SIGNIFICANCE\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"ACC\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"CHR\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"CHRPOS\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"SPDI\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"FXN_CLASS\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"VALIDATED\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"TAX_ID\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"CREATEDATE\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"UPDATEDATE\"]}\\t')\n",
    "    snps_fhandle.write(f'{snp_dict[\"DOCSUM\"]}\\t')\n",
    "\n",
    "    # parsing NP-variants\n",
    "    docsum_items = snp_dict[\"DOCSUM\"].split(\"|\")\n",
    "    hgvs_variants = docsum_items[0][5:].split(\",\")\n",
    "    np_variants = \",\".join([v for v in hgvs_variants if v.startswith(\"NP_\")])\n",
    "    snps_fhandle.write(f'{np_variants}\\t') # col:NP_VARIANTS\n",
    "    \n",
    "    # parsing ref and alt alleles\n",
    "    alleles = docsum_items[1][5:-1]\n",
    "    alleles = alleles.split(\"/\")\n",
    "    ref_allele = alleles[0]\n",
    "    alt_alleles = \",\".join(alleles[1:])\n",
    "    snps_fhandle.write(f'{ref_allele}\\t') # col:REF_ALLELE\n",
    "    snps_fhandle.write(f'{alt_alleles}\\t') # col:ALT_ALLELES\n",
    "    # print(alleles)\n",
    "    snps_fhandle.write(\"\\n\")\n",
    "\n",
    "    # parsing genes and putting into another .tsv file\n",
    "    genes = snp_dict[\"GENES\"][\"GENE_E\"]\n",
    "    if type(genes)!=list: genes = [genes]\n",
    "    for gene_dict in genes:\n",
    "        genes_fhandle.write(f'{snp_id}\\t') # col: SNP_ID\n",
    "        genes_fhandle.write(f'{gene_dict[\"NAME\"]}\\t') # col: GENE_NAME\n",
    "        genes_fhandle.write(f'{gene_dict[\"GENE_ID\"]}\\t')\n",
    "        genes_fhandle.write(\"\\n\")\n",
    "\n",
    "\n",
    "    # parsing global-mafs and putting into another .tsv file\n",
    "    mafs = snp_dict[\"GLOBAL_MAFS\"][\"MAF\"]\n",
    "    if type(mafs)!=list: mafs = [mafs]\n",
    "    for maf_dict in mafs:\n",
    "        global_mafs_fhandle.write(f'{snp_id}\\t') # col: SNP_ID\n",
    "        global_mafs_fhandle.write(f'{maf_dict[\"STUDY\"]}\\t')\n",
    "        global_mafs_fhandle.write(f'{maf_dict[\"FREQ\"]}\\t')\n",
    "        \n",
    "        freq_items = maf_dict[\"FREQ\"].split(\"=\")\n",
    "        alt_allele = freq_items[0]\n",
    "        alt_freq, alt_popu = freq_items[1].split(\"/\")\n",
    "\n",
    "        global_mafs_fhandle.write(f'{alt_allele}\\t') # col: ALT_ALLELE\t\t\n",
    "        global_mafs_fhandle.write(f'{alt_freq}\\t') # col: ALT_FREQ\n",
    "        global_mafs_fhandle.write(f'{alt_popu}\\t') # col: ALT_POPU\n",
    "        global_mafs_fhandle.write(\"\\n\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6866032"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = home_dir+\"data/dbsnp/search_results/6866032_snps.txt\"\n",
    "# filepath = home_dir+\"data/dbsnp/snps_search_result_small.xml\"\n",
    "with open(filepath) as f:\n",
    "    n_snps = 0\n",
    "    for i, line in enumerate(f.readlines()):\n",
    "        # print(line)\n",
    "        if line.startswith(\"<?xml\") or line.startswith(\"</ExchangeSet>\"): continue\n",
    "        elif line.startswith(\"<ExchangeSet\") :\n",
    "            if line.find(\"<DocumentSummary\")!=-1:\n",
    "                doc_sum = line[line.find(\"<DocumentSummary\"):]\n",
    "                parse_document_summary(doc_sum)\n",
    "                n_snps+=1\n",
    "\n",
    "        elif line.startswith(\"<DocumentSummary\"):\n",
    "            parse_document_summary(line)\n",
    "            n_snps+=1\n",
    "        else:\n",
    "            print(n_snps)\n",
    "            print(line)\n",
    "            break\n",
    "            # data_dict = xmltodict.parse(line)\n",
    "            # print(i, data_dict)\n",
    "        # if i==2: break\n",
    "        if i%100000==0: print(i) # just to see the progress\n",
    "\n",
    "n_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "snps_fhandle.close() # 7.6GB\n",
    "genes_fhandle.close() # 213MB\n",
    "global_mafs_fhandle.close() # 1.1G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = home_dir+\"data/dbsnp/snps_search_result.xml\"\n",
    "# filepath = home_dir+\"data/dbsnp/snps_search_result_small.xml\"\n",
    "# with open(filepath) as xml_file:\n",
    "#     data_dict = xmltodict.parse(xml_file.read())\n",
    "#     snps = data_dict[\"ExchangeSet\"][\"DocumentSummary\"]\n",
    "\n",
    "# print(\"#-downloaded snps:\", len(snps))\n",
    "# snps[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hopper_variant_effect_analysis_mine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
